{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'preprocessing' from 'c:\\\\Users\\\\HP\\\\Desktop\\\\TenX\\\\ethiomart\\\\scripts\\\\preprocessing.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0,os.path.abspath(\"../scripts\"))\n",
    "import preprocessing as pre\n",
    "import importlib\n",
    "importlib.reload(pre)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to input and output CSV files\n",
    "input_csv_path = '../data/telegram_data.csv'  # Update this with your actual path\n",
    "output_csv_path = '../data/cleaned_telegram_data.csv'\n",
    "\n",
    "# Clean the DataFrame by extracting only Amharic text\n",
    "pre.clean_dataframe_with_linebreaks(input_csv_path, output_csv_path)\n",
    "\n",
    "print(f\"Cleaned data saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Channel Title  Channel Username    ID  \\\n",
      "0  Standard kitchen  @standardkitchen  5988   \n",
      "1  Standard kitchen  @standardkitchen  5987   \n",
      "2  Standard kitchen  @standardkitchen  5985   \n",
      "3  Standard kitchen  @standardkitchen  5984   \n",
      "4  Standard kitchen  @standardkitchen  5981   \n",
      "\n",
      "                                             Message  \\\n",
      "0                                                      \n",
      "1   4 \\n\\n አነስተኛ የልብስ እና የጫማ ማጠቢያ ማሽን \\n ለቲሸርት፣ ለ...   \n",
      "2   \\n\\n \\n 385 \\n \\n \\n \\n \\n \\n\\n ዋጋ፦ 350 ብር\\n\\...   \n",
      "3                                                      \n",
      "4   ዘመናዊ የልብስ ማስቀመጫ ቁምሳጥን \\nአልቆል የተባላቹ ባለ 3ቱ ገብቶል...   \n",
      "\n",
      "                        Date                               Media Path  \n",
      "0  2024-09-26 17:17:50+00:00  ./data/photos\\@standardkitchen_5988.jpg  \n",
      "1  2024-09-26 08:13:33+00:00  ./data/photos\\@standardkitchen_5987.jpg  \n",
      "2  2024-09-25 07:14:37+00:00                                      NaN  \n",
      "3  2024-09-25 07:14:37+00:00  ./data/photos\\@standardkitchen_5984.jpg  \n",
      "4  2024-09-24 07:31:43+00:00  ./data/photos\\@standardkitchen_5981.jpg  \n"
     ]
    }
   ],
   "source": [
    "# Display the cleaned data\n",
    "cleaned_df = pd.read_csv(output_csv_path)\n",
    "print(cleaned_df.head())  # Show first few rows of the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/cleaned_telegram_data.csv') # Load your dataset\n",
    "pre.label_entities_conll_with_linebreaks(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ግዜ O\n",
      "እስከ O\n",
      "36ሊትር O\n",
      "ውሃ O\n",
      "ስለሚይዝ O\n",
      "ልብስ O\n",
      "በደንብ O\n",
      "አፅድቶ O\n",
      "ያጥባል O\n",
      "\n",
      "\n",
      "\n",
      "ዋጋ B-PRICE\n",
      "14 I-PRICE\n",
      "500 I-PRICE\n",
      "ብር I-PRICE\n",
      "\n",
      "\n",
      "\n",
      "1 O\n",
      "ፒያሳ B-LOC\n",
      "ጣይቱ I-LOC\n",
      "ሆቴል I-LOC\n",
      "ጊቢ I-LOC\n",
      "ውስጥ I-LOC\n",
      "ቢሮ I-LOC\n",
      "ቁ04 I-LOC\n",
      "\n",
      "\n",
      "2 O\n"
     ]
    }
   ],
   "source": [
    "# Displaying labeled data (first few tokens)\n",
    "with open('../data/labeled_data.conll', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "    for i, line in enumerate(lines[60:90]):  # Display the first 30 lines\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets have been successfully merged and labels normalized!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load all datasets \n",
    "dataset1 = pre.load_conll_dataset(\"../data/datasets/@classybrands_conll_output.conll\")\n",
    "dataset2 = pre.load_conll_dataset(\"../data/datasets/@mertteka_labeled_data.conll\")\n",
    "dataset3 = pre.load_conll_dataset(\"../data/datasets/conll_format_data.conll\")\n",
    "dataset4 = pre.load_conll_dataset(\"../data/datasets/final_Nejash_tokens_labels.conll\")\n",
    "dataset5 = pre.load_conll_dataset(\"../data/datasets/final_tokens_labels (1).conll\")\n",
    "dataset6 = pre.load_conll_dataset(\"../data/datasets/labeled_data (1).conll\")\n",
    "dataset7 = pre.load_conll_dataset(\"../data/datasets/labeled_data (2).conll\")\n",
    "#dataset8 = pre.load_conll_dataset(\"../data/datasets/labeled_data (3).conll\")\n",
    "dataset9 = pre.load_conll_dataset(\"../data/datasets/labeled_data (4).conll\")\n",
    "dataset10 = pre.load_conll_dataset(\"../data/datasets/labeled_data.conll\")\n",
    "dataset11 = pre.load_conll_dataset(\"../data/datasets/labeled_data_all_channels (1).conll\")\n",
    "dataset12= pre.load_conll_dataset(\"../data/datasets/labeled_data_conll (1).conll\")\n",
    "dataset13 = pre.load_conll_dataset(\"../data/datasets/labeled_data_conll.conll\")\n",
    "dataset14 = pre.load_conll_dataset(\"../data/datasets/labeled_data_freemarket_final.conll\")\n",
    "#dataset15 = pre.load_conll_dataset(\"../data/datasets/labeled_dataset.conll\")\n",
    "dataset16 = pre.load_conll_dataset(\"../data/datasets/labeled_helloomarket_conll (1).conll\")\n",
    "dataset17 = pre.load_conll_dataset(\"../data/datasets/labeled_messages.conll\")\n",
    "dataset18 = pre.load_conll_dataset(\"../data/datasets/labelled_subset.conll\")\n",
    "\n",
    "\n",
    "# Concatenate the datasets\n",
    "combined_dataset = pd.concat([dataset1, dataset2, dataset3, dataset4, dataset5, dataset6, dataset7, dataset9, dataset10, dataset11, dataset12, dataset13, dataset14, dataset16, dataset17, dataset18], ignore_index=True)\n",
    "\n",
    "# Normalize labels: convert all labels to lowercase\n",
    "combined_dataset[\"labels\"] = combined_dataset[\"labels\"].apply(lambda x: [label.lower() for label in x])\n",
    "\n",
    "# Save the combined dataset to a file (in CoNLL format)\n",
    "with open(\"../data/combined_dataset.conll\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for tokens, labels in zip(combined_dataset[\"tokens\"], combined_dataset[\"labels\"]):\n",
    "        for token, label in zip(tokens, labels):\n",
    "            f.write(f\"{token}\\t{label}\\n\")\n",
    "        f.write(\"\\n\")  # Separate sentences by a blank line\n",
    "\n",
    "print(\"Datasets have been successfully merged and labels normalized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
